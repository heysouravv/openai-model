{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt_index import SimpleDirectoryReader,GPTListIndex,GPTSimpleVectorIndex,LLMPredictor,PromptHelper\n",
    "from langchain.document_loaders.csv import CSVLoader\n",
    "from langchain import OpenAI\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/apple/Desktop/DODGEE/openai-model/data/Customer_Health_Exposure_Summary.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "def create_index(path):\n",
    "  max_input = 4096\n",
    "  tokens = 200\n",
    "  chunk_size = 600 #for LLM, we need to define chunk size\n",
    "  max_chunk_overlap = 20\n",
    "  \n",
    "  #define prompt\n",
    "  promptHelper = PromptHelper(max_input,tokens,max_chunk_overlap,chunk_size_limit=chunk_size)\n",
    "  \n",
    "  #define LLM — there could be many models we can use, but in this example, let’s go with OpenAI model\n",
    "  llmPredictor = LLMPredictor(llm=OpenAI(temperature=0, model_name=\"text-davinci-003\",max_tokens=tokens))\n",
    "  \n",
    "  #load data — it will take all the .txtx files, if there are more than 1\n",
    "  docs = CSVLoader(file_path='./data/Customer_Health_Exposure_Summary.csv').load()\n",
    "\n",
    "  #create vector index\n",
    "  vectorIndex = GPTSimpleVectorIndex(documents=docs,llm_predictor=llmPredictor,prompt_helper=promptHelper)\n",
    "  vectorIndex.save_to_disk('vectorIndex.json')\n",
    "  return vectorIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid document type: <class 'langchain.docstore.document.Document'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/apple/Desktop/DODGEE/openai-model/playground.ipynb Cell 4\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/apple/Desktop/DODGEE/openai-model/playground.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m vectorIndex \u001b[39m=\u001b[39m create_index(path)\n",
      "\u001b[1;32m/Users/apple/Desktop/DODGEE/openai-model/playground.ipynb Cell 4\u001b[0m in \u001b[0;36mcreate_index\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/Desktop/DODGEE/openai-model/playground.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m docs \u001b[39m=\u001b[39m CSVLoader(file_path\u001b[39m=\u001b[39mpath)\u001b[39m.\u001b[39mload()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/Desktop/DODGEE/openai-model/playground.ipynb#W5sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m#create vector index\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/apple/Desktop/DODGEE/openai-model/playground.ipynb#W5sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m vectorIndex \u001b[39m=\u001b[39m GPTSimpleVectorIndex(documents\u001b[39m=\u001b[39;49mdocs,llm_predictor\u001b[39m=\u001b[39;49mllmPredictor,prompt_helper\u001b[39m=\u001b[39;49mpromptHelper)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/Desktop/DODGEE/openai-model/playground.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m vectorIndex\u001b[39m.\u001b[39msave_to_disk(\u001b[39m'\u001b[39m\u001b[39mvectorIndex.json\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/apple/Desktop/DODGEE/openai-model/playground.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mreturn\u001b[39;00m vectorIndex\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gpt_index/indices/vector_store/vector_indices.py:84\u001b[0m, in \u001b[0;36mGPTSimpleVectorIndex.__init__\u001b[0;34m(self, documents, index_struct, text_qa_template, llm_predictor, embed_model, simple_vector_store_data_dict, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39m\"\"\"Init params.\"\"\"\u001b[39;00m\n\u001b[1;32m     80\u001b[0m vector_store \u001b[39m=\u001b[39m SimpleVectorStore(\n\u001b[1;32m     81\u001b[0m     simple_vector_store_data_dict\u001b[39m=\u001b[39msimple_vector_store_data_dict\n\u001b[1;32m     82\u001b[0m )\n\u001b[0;32m---> 84\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     85\u001b[0m     documents\u001b[39m=\u001b[39;49mdocuments,\n\u001b[1;32m     86\u001b[0m     index_struct\u001b[39m=\u001b[39;49mindex_struct,\n\u001b[1;32m     87\u001b[0m     text_qa_template\u001b[39m=\u001b[39;49mtext_qa_template,\n\u001b[1;32m     88\u001b[0m     llm_predictor\u001b[39m=\u001b[39;49mllm_predictor,\n\u001b[1;32m     89\u001b[0m     embed_model\u001b[39m=\u001b[39;49membed_model,\n\u001b[1;32m     90\u001b[0m     vector_store\u001b[39m=\u001b[39;49mvector_store,\n\u001b[1;32m     91\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     92\u001b[0m )\n\u001b[1;32m     94\u001b[0m \u001b[39m# TODO: Temporary hack to also store embeddings in index_struct\u001b[39;00m\n\u001b[1;32m     95\u001b[0m embedding_dict \u001b[39m=\u001b[39m vector_store\u001b[39m.\u001b[39m_data\u001b[39m.\u001b[39membedding_dict\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gpt_index/indices/vector_store/base.py:63\u001b[0m, in \u001b[0;36mGPTVectorStoreIndex.__init__\u001b[0;34m(self, documents, index_struct, text_qa_template, llm_predictor, embed_model, vector_store, text_splitter, use_async, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtext_qa_template \u001b[39m=\u001b[39m text_qa_template \u001b[39mor\u001b[39;00m DEFAULT_TEXT_QA_PROMPT\n\u001b[1;32m     62\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_async \u001b[39m=\u001b[39m use_async\n\u001b[0;32m---> 63\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     64\u001b[0m     documents\u001b[39m=\u001b[39;49mdocuments,\n\u001b[1;32m     65\u001b[0m     index_struct\u001b[39m=\u001b[39;49mindex_struct,\n\u001b[1;32m     66\u001b[0m     llm_predictor\u001b[39m=\u001b[39;49mllm_predictor,\n\u001b[1;32m     67\u001b[0m     embed_model\u001b[39m=\u001b[39;49membed_model,\n\u001b[1;32m     68\u001b[0m     text_splitter\u001b[39m=\u001b[39;49mtext_splitter,\n\u001b[1;32m     69\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m     70\u001b[0m )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gpt_index/indices/base.py:104\u001b[0m, in \u001b[0;36mBaseGPTIndex.__init__\u001b[0;34m(self, documents, index_struct, llm_predictor, embed_model, docstore, index_registry, prompt_helper, text_splitter, chunk_size_limit, include_extra_info)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     documents \u001b[39m=\u001b[39m cast(Sequence[DOCUMENTS_INPUT], documents)\n\u001b[0;32m--> 104\u001b[0m     documents \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_documents(\n\u001b[1;32m    105\u001b[0m         documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_docstore, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_index_registry\n\u001b[1;32m    106\u001b[0m     )\n\u001b[1;32m    107\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_documents(documents)\n\u001b[1;32m    108\u001b[0m     \u001b[39m# TODO: introduce document store outside __init__ function\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/gpt_index/indices/base.py:182\u001b[0m, in \u001b[0;36mBaseGPTIndex._process_documents\u001b[0;34m(self, documents, docstore, index_registry)\u001b[0m\n\u001b[1;32m    180\u001b[0m         results\u001b[39m.\u001b[39mappend(doc)\n\u001b[1;32m    181\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInvalid document type: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(doc)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    183\u001b[0m \u001b[39mreturn\u001b[39;00m cast(List[BaseDocument], results)\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid document type: <class 'langchain.docstore.document.Document'>."
     ]
    }
   ],
   "source": [
    "vectorIndex = create_index(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answerMe(vectorIndex):\n",
    "  vIndex = GPTSimpleVectorIndex.load_from_disk(vectorIndex)\n",
    "  while True:\n",
    "    inputdata = input('Please ask: ')\n",
    "    response = vIndex.query(inputdata,response_mode='compact')\n",
    "    print(f'Response: {response} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:> [query] Total LLM token usage: 650 tokens\n",
      "INFO:root:> [query] Total embedding token usage: 10 tokens\n"
     ]
    }
   ],
   "source": [
    "answerMe('vectorIndex.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
